{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Multiclass Classification Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Any # para docstrings y typing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion sigmoide\n",
    "def h_teta(matriz_t, matriz_x): # esta funcion es sigmoide(h_zeta)\n",
    "    #zeta_teta = np.matmul(vector_t.T, vector_x)\n",
    "    zeta_teta = np.matmul(matriz_t, matriz_x)\n",
    "    #print('Zeta teta: ', zeta_teta) # si zeta >= 0; entonces clase positiva.\n",
    "    #[5,4,-1] = 5 + 4x1-x2 = 0\n",
    "    # x2 =  5 + 4x1\n",
    "\n",
    "    return ((1 + np.power(np.e, -1*zeta_teta))**-1) # (n,1)\n",
    "\n",
    "def jota_teta(x, y, hipotesis, m, tetas):\n",
    "    #h = hipotesis(tetas, x)\n",
    "    #print('hipotesis: {}\\n'.format(-np.log(h)))\n",
    "    #print('y shape: {}\\n'.format(y.shape))\n",
    "    #print('h shape: {}\\n'.format(h.shape))\n",
    "    return (-1/float(m)) * (np.matmul(y.T, np.log(hipotesis) + np.matmul((1-y.T), np.log(1 - hipotesis))))\n",
    "\n",
    "def gradiente(\n",
    "        x,\n",
    "        y,\n",
    "        h,\n",
    "        m,\n",
    "        tetas,\n",
    "    ):\n",
    "    return (((np.matmul((h - y).T, x).T) / float(m)))\n",
    "\n",
    "def descenso_gradiente(\n",
    "        x_set: List[List[float]],\n",
    "        y_set: List[float],\n",
    "        #tetas_iniciales: List[float],\n",
    "        hipotesis: Callable[[Any], Any],#[[List[float], List[float]], List[float]], # Callable[[parametros], resultado]\n",
    "        gradiente: Callable[[Any], Any],#[[List[float], List[float], List[float], float], List[float]], # Callable[[parametros], resultado]\n",
    "        max_iters: int = 10000,\n",
    "        alpha: float = 0.0001,\n",
    "        _lambda: float = 0.0,\n",
    "        grado: int = 1\n",
    "    ) -> List[float]:\n",
    "    \n",
    "    \"\"\"Esta función ejecuta el descenso en gradiente para encontrar las tetas que minimizan el costo.\"\"\"\n",
    "    \n",
    "    unos = np.ones(x_set.shape[1]) # [1] ya que X viene en formato de filas, por lo que cada columna es una observacion.\n",
    "    \n",
    "    X = transformar_arreglo(x_set, grado)\n",
    "    \n",
    "    m, n = X.shape\n",
    "    #y_set = y_set.reshape(m,1) # convertir a vector columna.\n",
    "    #print(y_set[-10:])\n",
    "    tetas = np.random.rand(n,1)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        h = hipotesis(X, tetas) # vector solucion (100,1)\n",
    "        #print((h - ys).shape) # (100,1) - (100,1)\n",
    "        tetas -= alpha * gradiente(X, y_set, h, m, tetas) \n",
    "    \n",
    "    #costo = jota_teta(y_set, h, m)\n",
    "    #y_pred = np.matmul(X,tetas)\n",
    "\n",
    "    #return y_pred, tetas, costo.sum()\n",
    "    return tetas # retorno X, ya que incluye la col de uno's, la cual será útil en cross validation.\n",
    "\n",
    "def cross_validate(x_train, y_train, x_test, y_test, tetas):\n",
    "    \"\"\" Calculo la validación cruzada para los tetas resultantes del train set sobre el test set.\"\"\"\n",
    "    \n",
    "    m = x_train.shape[0]\n",
    "    h_train = h_teta(x_train, tetas)\n",
    "    h_test = h_teta(x_test, tetas)\n",
    "    costo_train = jota_teta(x_train, y_train, h_train, m, tetas)\n",
    "    costo_test = jota_teta(x_train, y_test, h_test, m, tetas)\n",
    "    \n",
    "    return [(costo_train, costo_test), (h_train, h_test)]\n",
    "\n",
    "def transformar_bias(x_set, grado):\n",
    "    \n",
    "    unos = np.ones(x_set.shape[1]) # [1] ya que X viene en formato de filas, por lo que cada columna es una observacion.\n",
    "    \n",
    "    if grado == 1:\n",
    "        X = np.vstack(\n",
    "            (\n",
    "            unos,\n",
    "            x_set,\n",
    "            #-x_set**2\n",
    "            )\n",
    "        ).T # Se transpuso la matriz para tener la columna de unos y asi calcular teta_0\n",
    "    elif grado == 2:\n",
    "        X = np.vstack(\n",
    "            (\n",
    "            unos,\n",
    "            x_set,\n",
    "            x_set**2\n",
    "            )\n",
    "        ).T\n",
    "    elif grado == -2:\n",
    "        X = np.vstack(\n",
    "            (\n",
    "            unos,\n",
    "            x_set,\n",
    "            -x_set**2\n",
    "            )\n",
    "        ).T\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.array([[1,2,3],[4,5,6],[7,8,9], [1,4,7]])\n",
    "X = np.array([\n",
    "    [1,4,7,1,0], # cada fila son los valores para todas las observaciones de una feature\n",
    "    [2,5,8,4,0],\n",
    "    [3,6,9,7,0],\n",
    "])\n",
    "#Y = np.array([[1,0,0], [0,1,0],[0,0,1], [0,0,0]])\n",
    "Y = np.array([\n",
    "    [1,0,0,0,0], # cada fila son los valores para todas las observaciones de una feature\n",
    "    [0,1,0,0,0],\n",
    "    [0,0,1,0,0],\n",
    "])\n",
    "#X = np.array([[1],[2],[3]])\n",
    "#Y = np.array([[1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # 4 observaciones, 3 features.\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 3, 3]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_HIDDEN_LAYERS = 2\n",
    "NN_ARCH = [X.shape[0], 4, 3, Y.shape[0]]\n",
    "NN_ARCH # Cuantas neuronas deseo que tenga cada capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30975814, 0.7022761 , 0.67219334, 0.16710222],\n",
       "       [0.29845537, 0.84663045, 0.5778933 , 0.56890888],\n",
       "       [0.24952919, 0.34340312, 0.44557254, 0.80503592],\n",
       "       [0.90758975, 0.15634901, 0.34210286, 0.34536396]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00990474, 0.64827585, 0.63373462, 0.0757352 ],\n",
       "        [0.89126963, 0.37618299, 0.26208752, 0.66950716],\n",
       "        [0.85141703, 0.27772932, 0.03914973, 0.96008971],\n",
       "        [0.52861848, 0.635508  , 0.57933768, 0.8462869 ]]),\n",
       " array([[0.41818099, 0.42775091, 0.00427627, 0.89723039, 0.02509069],\n",
       "        [0.87210105, 0.11351693, 0.82855083, 0.32620398, 0.20191474],\n",
       "        [0.0334966 , 0.47473493, 0.89216888, 0.79920142, 0.21185361]]),\n",
       " array([[0.94771751, 0.68469799, 0.1752419 , 0.65394532],\n",
       "        [0.41925688, 0.06778615, 0.43341854, 0.56638286],\n",
       "        [0.14866472, 0.68260659, 0.42323183, 0.28512791]])]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TETAS = [\n",
    "    np.random.rand(\n",
    "        NN_ARCH[i+1], # cantidad de neuronas en siguiente capa.\n",
    "        NN_ARCH[i]+1 # + 1 por la neurona del bias\n",
    "    ) for i in range(len(NN_ARCH)-1) # No considero la última capa.\n",
    "]\n",
    "TETAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuronas de salida | Neuronas de entrada (con bias)\n",
      "(4, 4)\n",
      "(3, 5)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Neuronas de salida | Neuronas de entrada (con bias)')\n",
    "for tetas in TETAS:\n",
    "    print(tetas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 4., 7., 1., 0.],\n",
       "       [2., 5., 8., 4., 0.],\n",
       "       [3., 6., 9., 7., 0.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z0 = transformar_bias(X,1).T\n",
    "z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4), (3, 5))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TETAS[0].shape, TETAS[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00],\n",
       "       [2.15285544e+00, 6.22609247e+00, 1.02993295e+01, 3.72326549e+00,\n",
       "        9.90474421e-03],\n",
       "       [3.80014913e+00, 7.72348214e+00, 1.16468152e+01, 7.00235280e+00,\n",
       "        8.91269631e-01],\n",
       "       [4.08771492e+00, 7.91862119e+00, 1.17495275e+01, 8.00637320e+00,\n",
       "        8.51417028e-01],\n",
       "       [4.86166252e+00, 1.10450602e+01, 1.72284580e+01, 9.40548545e+00,\n",
       "        5.28618475e-01]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = transformar_bias(np.matmul(TETAS[0], z0),1).T\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ],\n",
       "       [ 5.14492184, 10.49638115, 15.84784046,  9.4603066 ,  1.2034097 ],\n",
       "       [ 6.58017348, 12.79141164, 19.0026498 , 11.60737683,  1.99615908],\n",
       "       [ 8.7427754 , 18.54839975, 28.35402411, 16.4396329 ,  1.62580519]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = transformar_bias(np.matmul(TETAS[1], z1),1).T\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.34085437, 22.50579921, 33.67074404, 20.2098903 ,  3.18468813],\n",
       "       [ 8.57173863, 17.18029678, 25.78885494, 15.40251324,  2.28683194],\n",
       "       [ 8.9383704 , 18.01596266, 27.09355491, 16.20634184,  2.2785206 ]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#z3 = transformar_arreglo(np.matmul(TETAS[2], z2),1).T\n",
    "# la ultima capa no se le agrega bias porque es la salida, ya no sera entrada de ninguna otra capa mas.\n",
    "z3 = np.matmul(TETAS[2], z2)\n",
    "z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(T, X, Y):\n",
    "    \"\"\" Voy a devolver una matriz con los valores de activación para cada capa.\"\"\"\n",
    "    # X no tiene que entrar con bias.\n",
    "    A = [X] # z1 es igual al vector de entradas\n",
    "    for i in range(len(TETAS)):\n",
    "        A.append(\n",
    "            h_teta(\n",
    "                T[i],\n",
    "                A[i]\n",
    "                \n",
    "            )\n",
    "        )\n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-d52d1d146150>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTETAS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-227-ff5ae7c68220>\u001b[0m in \u001b[0;36mfeed_forward\u001b[1;34m(T, X, Y)\u001b[0m\n\u001b[0;32m      7\u001b[0m             h_teta(\n\u001b[0;32m      8\u001b[0m                 \u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTETAS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformar_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 4)"
     ]
    }
   ],
   "source": [
    "feed_forward(TETAS, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.15285544e+00, 6.22609247e+00, 1.02993295e+01, 3.72326549e+00,\n",
       "        9.90474421e-03],\n",
       "       [3.80014913e+00, 7.72348214e+00, 1.16468152e+01, 7.00235280e+00,\n",
       "        8.91269631e-01],\n",
       "       [4.08771492e+00, 7.91862119e+00, 1.17495275e+01, 8.00637320e+00,\n",
       "        8.51417028e-01],\n",
       "       [4.86166252e+00, 1.10450602e+01, 1.72284580e+01, 9.40548545e+00,\n",
       "        5.28618475e-01]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x = np.matmul(TETAS[0], transformar_bias(X,1).T)\n",
    "input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.44359386, 10.92341909, 16.40324432, 10.36257645,  1.15749416],\n",
       "       [ 7.67458307, 17.92468844, 28.17479382, 14.34800981,  0.92116791],\n",
       "       [ 7.71605569, 18.36029911, 29.00454253, 14.45836742,  0.79681868],\n",
       "       [10.03559289, 22.13440756, 34.23322223, 19.01637089,  1.51226567]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_z1 = np.matmul(TETAS[0], input_x)\n",
    "h_z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9956947 , 0.99998197, 0.99999992, 0.99996841, 0.76087709],\n",
       "       [0.99953573, 0.99999998, 1.        , 0.99999941, 0.71528001],\n",
       "       [0.99955458, 0.99999999, 1.        , 0.99999947, 0.68929355],\n",
       "       [0.99995619, 1.        , 1.        , 0.99999999, 0.81939674]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((1 + np.power(np.e, -1*h_z1))**-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
