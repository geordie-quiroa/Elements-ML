{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Multiclass Classification Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Any # para docstrings y typing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion sigmoide\n",
    "def h_teta(matriz_t, matriz_x): # esta funcion es sigmoide(h_zeta)\n",
    "    #zeta_teta = np.matmul(vector_t.T, vector_x)\n",
    "    zeta_teta = np.matmul(matriz_t, transformar_bias(matriz_x,1).T)\n",
    "    #print('Zeta teta: ', zeta_teta) # si zeta >= 0; entonces clase positiva.\n",
    "    #[5,4,-1] = 5 + 4x1-x2 = 0\n",
    "    # x2 =  5 + 4x1\n",
    "\n",
    "    return ((1 + np.power(np.e, -1*zeta_teta))**-1) # (n,1)\n",
    "\n",
    "def jota_teta(x, y, hipotesis, m, tetas):\n",
    "    #h = hipotesis(tetas, x)\n",
    "    #print('hipotesis: {}\\n'.format(-np.log(h)))\n",
    "    #print('y shape: {}\\n'.format(y.shape))\n",
    "    #print('h shape: {}\\n'.format(h.shape))\n",
    "    return (-1/float(m)) * (np.matmul(y.T, np.log(hipotesis) + np.matmul((1-y.T), np.log(1 - hipotesis))))\n",
    "\n",
    "def gradiente(\n",
    "        x,\n",
    "        y,\n",
    "        h,\n",
    "        m,\n",
    "        tetas,\n",
    "    ):\n",
    "    return (((np.matmul((h - y).T, x).T) / float(m)))\n",
    "\n",
    "def descenso_gradiente(\n",
    "        x_set: List[List[float]],\n",
    "        y_set: List[float],\n",
    "        #tetas_iniciales: List[float],\n",
    "        hipotesis: Callable[[Any], Any],#[[List[float], List[float]], List[float]], # Callable[[parametros], resultado]\n",
    "        gradiente: Callable[[Any], Any],#[[List[float], List[float], List[float], float], List[float]], # Callable[[parametros], resultado]\n",
    "        max_iters: int = 10000,\n",
    "        alpha: float = 0.0001,\n",
    "        _lambda: float = 0.0,\n",
    "        grado: int = 1\n",
    "    ) -> List[float]:\n",
    "    \n",
    "    \"\"\"Esta función ejecuta el descenso en gradiente para encontrar las tetas que minimizan el costo.\"\"\"\n",
    "    \n",
    "    unos = np.ones(x_set.shape[1]) # [1] ya que X viene en formato de filas, por lo que cada columna es una observacion.\n",
    "    \n",
    "    X = transformar_arreglo(x_set, grado)\n",
    "    \n",
    "    m, n = X.shape\n",
    "    #y_set = y_set.reshape(m,1) # convertir a vector columna.\n",
    "    #print(y_set[-10:])\n",
    "    tetas = np.random.rand(n,1)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        h = hipotesis(X, tetas) # vector solucion (100,1)\n",
    "        #print((h - ys).shape) # (100,1) - (100,1)\n",
    "        tetas -= alpha * gradiente(X, y_set, h, m, tetas) \n",
    "    \n",
    "    #costo = jota_teta(y_set, h, m)\n",
    "    #y_pred = np.matmul(X,tetas)\n",
    "\n",
    "    #return y_pred, tetas, costo.sum()\n",
    "    return tetas # retorno X, ya que incluye la col de uno's, la cual será útil en cross validation.\n",
    "\n",
    "def cross_validate(x_train, y_train, x_test, y_test, tetas):\n",
    "    \"\"\" Calculo la validación cruzada para los tetas resultantes del train set sobre el test set.\"\"\"\n",
    "    \n",
    "    m = x_train.shape[0]\n",
    "    h_train = h_teta(x_train, tetas)\n",
    "    h_test = h_teta(x_test, tetas)\n",
    "    costo_train = jota_teta(x_train, y_train, h_train, m, tetas)\n",
    "    costo_test = jota_teta(x_train, y_test, h_test, m, tetas)\n",
    "    \n",
    "    return [(costo_train, costo_test), (h_train, h_test)]\n",
    "\n",
    "def transformar_bias(x_set, grado):\n",
    "    \n",
    "    unos = np.ones(x_set.shape[1]) # [1] ya que X viene en formato de filas, por lo que cada columna es una observacion.\n",
    "    \n",
    "    if grado == 1:\n",
    "        X = np.vstack(\n",
    "            (\n",
    "            unos,\n",
    "            x_set,\n",
    "            #-x_set**2\n",
    "            )\n",
    "        ).T # Se transpuso la matriz para tener la columna de unos y asi calcular teta_0\n",
    "    elif grado == 2:\n",
    "        X = np.vstack(\n",
    "            (\n",
    "            unos,\n",
    "            x_set,\n",
    "            x_set**2\n",
    "            )\n",
    "        ).T\n",
    "    elif grado == -2:\n",
    "        X = np.vstack(\n",
    "            (\n",
    "            unos,\n",
    "            x_set,\n",
    "            -x_set**2\n",
    "            )\n",
    "        ).T\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.array([[1,2,3],[4,5,6],[7,8,9], [1,4,7]])\n",
    "X = np.array([\n",
    "    [1,4,7,1,0], # cada fila son los valores para todas las observaciones de una feature\n",
    "    [2,5,8,4,0],\n",
    "    [3,6,9,7,0],\n",
    "])\n",
    "#Y = np.array([[1,0,0], [0,1,0],[0,0,1], [0,0,0]])\n",
    "Y = np.array([\n",
    "    [1,0,0,0,0], # cada fila son los valores para todas las observaciones de una feature\n",
    "    [0,1,0,0,0],\n",
    "    [0,0,1,0,0],\n",
    "])\n",
    "#X = np.array([[1],[2],[3]])\n",
    "#Y = np.array([[1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # 4 observaciones, 3 features.\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 3, 3]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_HIDDEN_LAYERS = 2\n",
    "NN_ARCH = [X.shape[0], 4, 3, Y.shape[0]] # X y Y tienen que estar transpuestas para utilizar X[0] y no X[1]\n",
    "NN_ARCH # Cuantas neuronas deseo que tenga cada capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30975814, 0.7022761 , 0.67219334, 0.16710222],\n",
       "       [0.29845537, 0.84663045, 0.5778933 , 0.56890888],\n",
       "       [0.24952919, 0.34340312, 0.44557254, 0.80503592],\n",
       "       [0.90758975, 0.15634901, 0.34210286, 0.34536396]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00990474, 0.64827585, 0.63373462, 0.0757352 ],\n",
       "        [0.89126963, 0.37618299, 0.26208752, 0.66950716],\n",
       "        [0.85141703, 0.27772932, 0.03914973, 0.96008971],\n",
       "        [0.52861848, 0.635508  , 0.57933768, 0.8462869 ]]),\n",
       " array([[0.41818099, 0.42775091, 0.00427627, 0.89723039, 0.02509069],\n",
       "        [0.87210105, 0.11351693, 0.82855083, 0.32620398, 0.20191474],\n",
       "        [0.0334966 , 0.47473493, 0.89216888, 0.79920142, 0.21185361]]),\n",
       " array([[0.94771751, 0.68469799, 0.1752419 , 0.65394532],\n",
       "        [0.41925688, 0.06778615, 0.43341854, 0.56638286],\n",
       "        [0.14866472, 0.68260659, 0.42323183, 0.28512791]])]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TETAS = [\n",
    "    np.random.rand(\n",
    "        NN_ARCH[i+1], # cantidad de neuronas en siguiente capa.\n",
    "        NN_ARCH[i]+1 # cantidad de neuronas de entrada, + 1 por la neurona del bias\n",
    "    ) for i in range(len(NN_ARCH)-1) # No considero la última capa, porque es la hipotesis.\n",
    "]\n",
    "TETAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuronas de salida | Neuronas de entrada (con bias)\n",
      "(4, 4)\n",
      "(3, 5)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Neuronas de salida | Neuronas de entrada (con bias)')\n",
    "for tetas in TETAS:\n",
    "    print(tetas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 4., 7., 1., 0.],\n",
       "       [2., 5., 8., 4., 0.],\n",
       "       [3., 6., 9., 7., 0.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z0 = transformar_bias(X,1).T\n",
    "z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4), (3, 5))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TETAS[0].shape, TETAS[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00],\n",
       "       [2.15285544e+00, 6.22609247e+00, 1.02993295e+01, 3.72326549e+00,\n",
       "        9.90474421e-03],\n",
       "       [3.80014913e+00, 7.72348214e+00, 1.16468152e+01, 7.00235280e+00,\n",
       "        8.91269631e-01],\n",
       "       [4.08771492e+00, 7.91862119e+00, 1.17495275e+01, 8.00637320e+00,\n",
       "        8.51417028e-01],\n",
       "       [4.86166252e+00, 1.10450602e+01, 1.72284580e+01, 9.40548545e+00,\n",
       "        5.28618475e-01]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = transformar_bias(np.matmul(TETAS[0], z0),1).T\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ],\n",
       "       [ 5.14492184, 10.49638115, 15.84784046,  9.4603066 ,  1.2034097 ],\n",
       "       [ 6.58017348, 12.79141164, 19.0026498 , 11.60737683,  1.99615908],\n",
       "       [ 8.7427754 , 18.54839975, 28.35402411, 16.4396329 ,  1.62580519]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = transformar_bias(np.matmul(TETAS[1], z1),1).T\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.34085437, 22.50579921, 33.67074404, 20.2098903 ,  3.18468813],\n",
       "       [ 8.57173863, 17.18029678, 25.78885494, 15.40251324,  2.28683194],\n",
       "       [ 8.9383704 , 18.01596266, 27.09355491, 16.20634184,  2.2785206 ]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#z3 = transformar_arreglo(np.matmul(TETAS[2], z2),1).T\n",
    "# la ultima capa no se le agrega bias porque es la salida, ya no sera entrada de ninguna otra capa mas.\n",
    "z3 = np.matmul(TETAS[2], z2)\n",
    "z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(T, X):\n",
    "    \"\"\" Voy a devolver una matriz con los valores de activación para cada capa.\"\"\"\n",
    "    # X no tiene que entrar con bias.\n",
    "    A = [X] # z0 es igual al vector de entradas\n",
    "    for i in range(len(TETAS)):\n",
    "        A.append(\n",
    "            # h_teta es la fx de activacion; sigmoide.\n",
    "            h_teta(\n",
    "                T[i],\n",
    "                A[i] # en h_teta le agrego el bias a la matriz\n",
    "            )\n",
    "        )\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capa  0\n",
      "capa  1\n",
      "capa  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[1, 4, 7, 1, 0],\n",
       "        [2, 5, 8, 4, 0],\n",
       "        [3, 6, 9, 7, 0]]),\n",
       " array([[0.89593531, 0.99802674, 0.99996635, 0.97641474, 0.50247617],\n",
       "        [0.97812192, 0.99955788, 0.99999125, 0.99909109, 0.70915211],\n",
       "        [0.98349931, 0.99963623, 0.99999211, 0.99966678, 0.70086431],\n",
       "        [0.9923218 , 0.99998403, 0.99999997, 0.99991774, 0.62916084]]),\n",
       " array([[0.8472152 , 0.85462628, 0.85476926, 0.85347694, 0.78258099],\n",
       "        [0.90932446, 0.91226226, 0.91231815, 0.91203439, 0.86673235],\n",
       "        [0.91114687, 0.91757383, 0.9176944 , 0.91676354, 0.83175893]]),\n",
       " array([[0.90745895, 0.90827799, 0.90829353, 0.90816489, 0.89838083],\n",
       "        [0.80009087, 0.80095569, 0.80097198, 0.80085434, 0.78902476],\n",
       "        [0.79764303, 0.79895287, 0.79897786, 0.79877419, 0.78361287]])]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = feed_forward(TETAS, X, Y)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90745895, 0.90827799, 0.90829353, 0.90816489, 0.89838083],\n",
       "       [0.80009087, 0.80095569, 0.80097198, 0.80085434, 0.78902476],\n",
       "       [0.79764303, 0.79895287, 0.79897786, 0.79877419, 0.78361287]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_obs = X.shape[1]\n",
    "n_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00990474, 0.64827585, 0.63373462, 0.0757352 ],\n",
       "        [0.89126963, 0.37618299, 0.26208752, 0.66950716],\n",
       "        [0.85141703, 0.27772932, 0.03914973, 0.96008971],\n",
       "        [0.52861848, 0.635508  , 0.57933768, 0.8462869 ]]),\n",
       " array([[0.41818099, 0.42775091, 0.00427627, 0.89723039, 0.02509069],\n",
       "        [0.87210105, 0.11351693, 0.82855083, 0.32620398, 0.20191474],\n",
       "        [0.0334966 , 0.47473493, 0.89216888, 0.79920142, 0.21185361]]),\n",
       " array([[0.94771751, 0.68469799, 0.1752419 , 0.65394532],\n",
       "        [0.41925688, 0.06778615, 0.43341854, 0.56638286],\n",
       "        [0.14866472, 0.68260659, 0.42323183, 0.28512791]])]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TETAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00990474, 0.64827585, 0.63373462, 0.0757352 , 0.89126963,\n",
       "       0.37618299, 0.26208752, 0.66950716, 0.85141703, 0.27772932,\n",
       "       0.03914973, 0.96008971, 0.52861848, 0.635508  , 0.57933768,\n",
       "       0.8462869 , 0.41818099, 0.42775091, 0.00427627, 0.89723039,\n",
       "       0.02509069, 0.87210105, 0.11351693, 0.82855083, 0.32620398,\n",
       "       0.20191474, 0.0334966 , 0.47473493, 0.89216888, 0.79920142,\n",
       "       0.21185361, 0.94771751, 0.68469799, 0.1752419 , 0.65394532,\n",
       "       0.41925688, 0.06778615, 0.43341854, 0.56638286, 0.14866472,\n",
       "       0.68260659, 0.42323183, 0.28512791])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten all arrays\n",
    "flat = np.concatenate([TETAS[i].flatten() for i in range(len(TETAS))])\n",
    "flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 4), (3, 5), (3, 4)]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construir shapes de cada matriz de tetas\n",
    "shapes = [(NN_ARCH[i+1], NN_ARCH[i]+1) for i in range(len(NN_ARCH)-1)]\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 15, 12]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = [np.product(shape) for shape in shapes]\n",
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum_block_size = np.zeros(len(NN_ARCH), dtype=int)\n",
    "accum_block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 16, 31, 43])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(block_size)):\n",
    "    accum_block_size[i+1] = accum_block_size[i] + block_size[i]\n",
    "accum_block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_tetas = [\n",
    "    flat[accum_block_size[i]:accum_block_size[i+1]].reshape(*shapes[i])\n",
    "    for i in range(len(shapes))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00990474, 0.64827585, 0.63373462, 0.0757352 ],\n",
       "        [0.89126963, 0.37618299, 0.26208752, 0.66950716],\n",
       "        [0.85141703, 0.27772932, 0.03914973, 0.96008971],\n",
       "        [0.52861848, 0.635508  , 0.57933768, 0.8462869 ]]),\n",
       " array([[0.41818099, 0.42775091, 0.00427627, 0.89723039, 0.02509069],\n",
       "        [0.87210105, 0.11351693, 0.82855083, 0.32620398, 0.20191474],\n",
       "        [0.0334966 , 0.47473493, 0.89216888, 0.79920142, 0.21185361]]),\n",
       " array([[0.94771751, 0.68469799, 0.1752419 , 0.65394532],\n",
       "        [0.41925688, 0.06778615, 0.43341854, 0.56638286],\n",
       "        [0.14866472, 0.68260659, 0.42323183, 0.28512791]])]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_tetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fx para aplanar matrices de tetas\n",
    "flat_tetas = lambda tetas_matrixes : np.concatenate(\n",
    "    [\n",
    "        tetas_matrixes[i].flatten()\n",
    "        for i in range(len(tetas_matrixes))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00990474, 0.64827585, 0.63373462, 0.0757352 , 0.89126963,\n",
       "       0.37618299, 0.26208752, 0.66950716, 0.85141703, 0.27772932,\n",
       "       0.03914973, 0.96008971, 0.52861848, 0.635508  , 0.57933768,\n",
       "       0.8462869 , 0.41818099, 0.42775091, 0.00427627, 0.89723039,\n",
       "       0.02509069, 0.87210105, 0.11351693, 0.82855083, 0.32620398,\n",
       "       0.20191474, 0.0334966 , 0.47473493, 0.89216888, 0.79920142,\n",
       "       0.21185361, 0.94771751, 0.68469799, 0.1752419 , 0.65394532,\n",
       "       0.41925688, 0.06778615, 0.43341854, 0.56638286, 0.14866472,\n",
       "       0.68260659, 0.42323183, 0.28512791])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_tetas(TETAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tetas_matrix(flat_tetas_array, nn_arch):\n",
    "    \"\"\" Transformo un arreglo de tetas, en un arreglo de matrices de tetas.\n",
    "    \n",
    "    flat_tetas_array: array unidimensional con los valores de tetas.\n",
    "    nn_arch: Architectura de la red neuronal, es un arreglo con la cantidad de neuronas en cada capa.\n",
    "    \n",
    "    Retorno un arreglo de matrices de tetas.\n",
    "    \"\"\"\n",
    "    layers = len(nn_arch)\n",
    "    \n",
    "    # construyo los shapes utilizando la arquitectura de la Red Neuronal.\n",
    "    # Shape = (Cantidad neuronas de salida, Cantidad neuronas entrada + 1) por el bias \n",
    "    shapes = [\n",
    "        (\n",
    "            nn_arch[i+1], # neuronas de salida en la sig. capa\n",
    "            nn_arch[i]+1 # neuronas de entrada mas neurona del bias\n",
    "        ) for i in range(layers - 1)\n",
    "    ]\n",
    "    \n",
    "    # Calculo el tamaño de cada matriz en el arreglo\n",
    "    block_size = [\n",
    "        np.product(shape)\n",
    "        for shape in shapes\n",
    "    ]\n",
    "    \n",
    "    accum_block_size = np.zeros(len(NN_ARCH), dtype=int) # indices para cada particion del flat array\n",
    "    \n",
    "    for i in range(len(block_size)):\n",
    "        accum_block_size[i+1] = accum_block_size[i] + block_size[i] # Calculo la suma acumulada de los indices\n",
    "    \n",
    "    return [\n",
    "        flat_tetas_array[\n",
    "            accum_block_size[i] : accum_block_size[i+1]\n",
    "        ].reshape(*shapes[i]) # reshapeo cada segmento del flat array a su shape correspondiente\n",
    "        for i in range(len(shapes))\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00990474, 0.64827585, 0.63373462, 0.0757352 ],\n",
       "        [0.89126963, 0.37618299, 0.26208752, 0.66950716],\n",
       "        [0.85141703, 0.27772932, 0.03914973, 0.96008971],\n",
       "        [0.52861848, 0.635508  , 0.57933768, 0.8462869 ]]),\n",
       " array([[0.41818099, 0.42775091, 0.00427627, 0.89723039, 0.02509069],\n",
       "        [0.87210105, 0.11351693, 0.82855083, 0.32620398, 0.20191474],\n",
       "        [0.0334966 , 0.47473493, 0.89216888, 0.79920142, 0.21185361]]),\n",
       " array([[0.94771751, 0.68469799, 0.1752419 , 0.65394532],\n",
       "        [0.41925688, 0.06778615, 0.43341854, 0.56638286],\n",
       "        [0.14866472, 0.68260659, 0.42323183, 0.28512791]])]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_tetas_matrix(flat, NN_ARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape  (4, 4)\n",
      "Shape  (3, 5)\n",
      "Shape  (3, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00990474, 0.64827585, 0.63373462, 0.0757352 , 0.89126963,\n",
       "       0.37618299, 0.26208752, 0.66950716, 0.85141703, 0.27772932,\n",
       "       0.03914973, 0.96008971, 0.52861848, 0.635508  , 0.57933768,\n",
       "       0.8462869 , 0.41818099, 0.42775091, 0.00427627, 0.89723039,\n",
       "       0.02509069, 0.87210105, 0.11351693, 0.82855083, 0.32620398,\n",
       "       0.20191474, 0.0334966 , 0.47473493, 0.89216888, 0.79920142,\n",
       "       0.21185361, 0.94771751, 0.68469799, 0.1752419 , 0.65394532,\n",
       "       0.41925688, 0.06778615, 0.43341854, 0.56638286, 0.14866472,\n",
       "       0.68260659, 0.42323183, 0.28512791])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "for i in range(len(TETAS)): # parecido a inflate matrix\n",
    "    print('Shape ', TETAS[i].shape)\n",
    "    t = np.append(t, TETAS[i])\n",
    "    #print(TETAS[i].flatten().reshape(\n",
    "    #    NN_ARCH[i+1],\n",
    "    #    NN_ARCH[i]+1))\n",
    "t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [5],\n",
       "       [6],\n",
       "       [3],\n",
       "       [4],\n",
       "       [0],\n",
       "       [7],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [7],\n",
       "       [0],\n",
       "       [6],\n",
       "       [4],\n",
       "       [2],\n",
       "       [3],\n",
       "       [1],\n",
       "       [8],\n",
       "       [7],\n",
       "       [1]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.randint(0,10, 20).reshape(20,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = (y == np.arange(10)).astype(int)\n",
    "Y.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90745895, 0.90827799, 0.90829353, 0.90816489, 0.89838083],\n",
       "       [0.80009087, 0.80095569, 0.80097198, 0.80085434, 0.78902476],\n",
       "       [0.79764303, 0.79895287, 0.79897786, 0.79877419, 0.78361287]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_LAYERS = len(NN_ARCH)\n",
    "NN_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " array([[-0.09254105,  0.90827799,  0.90829353,  0.90816489,  0.89838083],\n",
       "        [ 0.80009087, -0.19904431,  0.80097198,  0.80085434,  0.78902476],\n",
       "        [ 0.79764303,  0.79895287, -0.20102214,  0.79877419,  0.78361287]])]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [*range(NN_LAYERS-1), A[-1] - Y]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TETAS[2][:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(NN_LAYERS-2, -1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape TETAS[i][:,1:]  (3, 3)\n",
      "Shape Delta[i+1]  (3, 5)\n",
      "Shape A[i]  (3, 5)\n",
      "\n",
      "\n",
      "Shape TETAS[i][:,1:]  (4, 3)\n",
      "Shape Delta[i+1]  (3, 5)\n",
      "Shape A[i]  (4, 5)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(NN_LAYERS-2, 0, -1):\n",
    "    print('Shape TETAS[i][:,1:] ', TETAS[i][:, 1:].T.shape)\n",
    "    print('Shape Delta[i+1] ', D[i+1].shape)\n",
    "    print('Shape A[i] ', A[i].shape)\n",
    "    print('\\n')\n",
    "    D[i] = np.matmul(\n",
    "        TETAS[i][:, 1:].T, # (3, 3) Excluyo la neurona del bias.\n",
    "        D[i+1] # (3, m) m -> cantidad obs.\n",
    "    ) * A[i] * (1 - A[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64827585, 0.63373462, 0.0757352 ],\n",
       "       [0.37618299, 0.26208752, 0.66950716],\n",
       "       [0.27772932, 0.03914973, 0.96008971],\n",
       "       [0.635508  , 0.57933768, 0.8462869 ]])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TETAS[0][:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = [0 for teta in tetas]\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00990474, 0.64827585, 0.63373462, 0.0757352 ],\n",
       "        [0.89126963, 0.37618299, 0.26208752, 0.66950716],\n",
       "        [0.85141703, 0.27772932, 0.03914973, 0.96008971],\n",
       "        [0.52861848, 0.635508  , 0.57933768, 0.8462869 ]]),\n",
       " array([[0.41818099, 0.42775091, 0.00427627, 0.89723039, 0.02509069],\n",
       "        [0.87210105, 0.11351693, 0.82855083, 0.32620398, 0.20191474],\n",
       "        [0.0334966 , 0.47473493, 0.89216888, 0.79920142, 0.21185361]]),\n",
       " array([[0.94771751, 0.68469799, 0.1752419 , 0.65394532],\n",
       "        [0.41925688, 0.06778615, 0.43341854, 0.56638286],\n",
       "        [0.14866472, 0.68260659, 0.42323183, 0.28512791]])]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TETAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((np.ones((1,m), float),A[2])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = X.shape[1]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 5), (3, 5), (3, 4))"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[3].shape, A[2].shape, TETAS[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.53057619, 2.952371  , 3.18002391, 3.16243826],\n",
       "       [2.99189764, 2.49337336, 2.69098286, 2.6718824 ],\n",
       "       [2.97796082, 2.48172977, 2.67846689, 2.65945536]])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(\n",
    "            d[3], # diferencia capa siguiente (3,5)\n",
    "            np.vstack((np.ones((1,m), float), A[2])).T # ()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " array([[-0.09254105,  0.90827799,  0.90829353,  0.90816489,  0.89838083],\n",
       "        [ 0.80009087, -0.19904431,  0.80097198,  0.80085434,  0.78902476],\n",
       "        [ 0.79764303,  0.79895287, -0.20102214,  0.79877419,  0.78361287]])]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0, 0.2, 0.4,\n",
       "       array([[-0.01850821,  0.1816556 ,  0.18165871,  0.18163298,  0.17967617],\n",
       "       [ 0.16001817, -0.03980886,  0.1601944 ,  0.16017087,  0.15780495],\n",
       "       [ 0.15952861,  0.15979057, -0.04020443,  0.15975484,  0.15672257]])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.divide(d, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " array([[-0.09254105,  0.90827799,  0.90829353,  0.90816489,  0.89838083],\n",
       "        [ 0.80009087, -0.19904431,  0.80097198,  0.80085434,  0.78902476],\n",
       "        [ 0.79764303,  0.79895287, -0.20102214,  0.79877419,  0.78361287]])]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = len(NN_ARCH)\n",
    "d = [*range(layers - 1), A[-1] - Y] # -1 porque no tomo en cuenta la primera capa\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(T, nn_arch, X, Y):\n",
    "    \"\"\"Calculo la matriz del gradiente para las tetas de cada capa de la Red Neuronal.\n",
    "    \n",
    "    T: Arrgelo flat de todas las tetas de la Red Neuronal.\n",
    "    nn_arch: Arreglo de la cantidad de neuronas por capa.\n",
    "    \n",
    "    Retorno el gradiente para cada capa\n",
    "    \"\"\"\n",
    "    #Paso 1\n",
    "    D = [0.0 for teta in tetas] # Gradiente\n",
    "    \n",
    "    m = X.shape[1] # [1] porque cada columna es una observacion.\n",
    "    layers = len(nn_arch)\n",
    "    TETAS = build_tetas_matrix(T, nn_arch)\n",
    "    \n",
    "    # Paso 2.2\n",
    "    A = feed_forward(TETAS, X) # valores de activacion para cada neurona de cada capa\n",
    "    \n",
    "    # Calculo el error entre la hipotesis de la NN y los valores actuales.\n",
    "    d = [*range(layers - 1), A[-1] - Y] # -1 porque no tomo en cuenta la primera capa\n",
    "    \n",
    "    for l in range(layers-2, -1, -1): # -2 por capa de salida ya calculada, y capa de entrada. Solo para transitions.\n",
    "        #print('Shape TETAS[l][:,1:] ', TETAS[l][:, 1:].T.shape)\n",
    "        #print('Shape Delta[l+1] ', D[l+1].shape)\n",
    "        #print('Shape A[l] ', A[l].shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        # Paso 2.4\n",
    "        d[l] = np.matmul(\n",
    "            TETAS[l][:, 1:].T, # (3, 3) Excluyo la neurona del bias.\n",
    "            d[l+1] # (3, m) cantidad de neuronas, m -> cantidad obs.\n",
    "        ) * A[l] * (1 - A[l])\n",
    "        \n",
    "        # Paso 2.5 y 2.6\n",
    "        D[l] += np.matmul(\n",
    "            d[l+1], # diferencia capa siguiente (3,5)\n",
    "            np.vstack((np.ones((1,m), float), A[l])).T # ()\n",
    "        )\n",
    "        \n",
    "    return np.divide(D, m) # Paso 3 retorno la matriz de gradientes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[1.08011471e-02, 1.80862741e-03, 4.60575958e-03, 7.40289175e-03],\n",
       "       [1.02161033e-02, 4.42318820e-04, 9.15723448e-04, 1.38912808e-03],\n",
       "       [1.53348720e-02, 4.60577264e-04, 9.11728106e-04, 1.36287895e-03],\n",
       "       [2.92075022e-03, 3.67106846e-05, 7.44679322e-05, 1.12225180e-04]]),\n",
       "       array([[0.12741184, 0.10481613, 0.11515646, 0.11491141, 0.11211483],\n",
       "       [0.05708388, 0.0460527 , 0.05124362, 0.05115208, 0.04987841],\n",
       "       [0.09038983, 0.07135019, 0.079909  , 0.0796833 , 0.07725646]]),\n",
       "       array([[0.70611524, 0.5904742 , 0.63600478, 0.63248765],\n",
       "       [0.59837953, 0.49867467, 0.53819657, 0.53437648],\n",
       "       [0.59559216, 0.49634595, 0.53569338, 0.53189107]])], dtype=object)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previo a cambios\n",
    "back_propagation(flat, NN_ARCH, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego de cambios\n",
    "back_propagation(flat, NN_ARCH, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(3, 5)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "for tetas_matrix in back_propagation(flat, NN_ARCH, X, Y):\n",
    "    print(tetas_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(3, 5)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "for tetas_matrix in TETAS:\n",
    "    print(tetas_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70611524, 0.5904742 , 0.63600478, 0.63248765],\n",
       "       [0.59837953, 0.49867467, 0.53819657, 0.53437648],\n",
       "       [0.59559216, 0.49634595, 0.53569338, 0.53189107]])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TETAS -= back_propagation(flat, NN_ARCH, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fx para aplanar matrices de tetas\n",
    "flattenize_tetas = lambda tetas_matrixes : np.concatenate(\n",
    "    [\n",
    "        tetas_matrixes[i].flatten()\n",
    "        for i in range(len(tetas_matrixes))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tetas_matrix(flat_tetas_array, nn_arch):\n",
    "    \"\"\" Transformo un arreglo de tetas, en un arreglo de matrices de tetas.\n",
    "    \n",
    "    flat_tetas_array: array unidimensional con los valores de tetas.\n",
    "    nn_arch: Architectura de la red neuronal, es un arreglo con la cantidad de neuronas en cada capa.\n",
    "    \n",
    "    Retorno un arreglo de matrices de tetas.\n",
    "    \"\"\"\n",
    "    layers = len(nn_arch)\n",
    "    \n",
    "    # construyo los shapes utilizando la arquitectura de la Red Neuronal.\n",
    "    # Shape = (Cantidad neuronas de salida, Cantidad neuronas entrada + 1) por el bias \n",
    "    shapes = [\n",
    "        (\n",
    "            nn_arch[i+1], # neuronas de salida en la sig. capa\n",
    "            nn_arch[i]+1 # neuronas de entrada mas neurona del bias\n",
    "        ) for i in range(layers - 1)\n",
    "    ]\n",
    "    \n",
    "    # Calculo el tamaño de cada matriz en el arreglo\n",
    "    block_size = [\n",
    "        np.product(shape)\n",
    "        for shape in shapes\n",
    "    ]\n",
    "    \n",
    "    accum_block_size = np.zeros(len(NN_ARCH), dtype=int) # indices para cada particion del flat array\n",
    "    \n",
    "    for i in range(len(block_size)):\n",
    "        accum_block_size[i+1] = accum_block_size[i] + block_size[i] # Calculo la suma acumulada de los indices\n",
    "    \n",
    "    return [\n",
    "        flat_tetas_array[\n",
    "            accum_block_size[i] : accum_block_size[i+1]\n",
    "        ].reshape(*shapes[i]) # reshapeo cada segmento del flat array a su shape correspondiente\n",
    "        for i in range(len(shapes))\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(T, X):\n",
    "    \"\"\" Voy a devolver una matriz con los valores de activación para cada capa.\"\"\"\n",
    "    # X no tiene que entrar con bias.\n",
    "    A = [X] # a0 = z0 El vector de activacion en la capa 0 es igual al vector de entradas\n",
    "    for i in range(len(TETAS)):\n",
    "        A.append(\n",
    "            # h_teta es la fx de activacion; sigmoide.\n",
    "            h_teta(\n",
    "                T[i],\n",
    "                A[i] # en h_teta le agrego el bias a la matriz de activaciones\n",
    "            )\n",
    "        )\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(T, nn_arch, X, Y):\n",
    "    \"\"\"Calculo la matriz del gradiente para las tetas de cada capa de la Red Neuronal.\n",
    "    \n",
    "    T: Arrgelo flat de todas las tetas de la Red Neuronal.\n",
    "    nn_arch: Arreglo de la cantidad de neuronas por capa.\n",
    "    \n",
    "    Retorno el gradiente para las tetas de cada capa.\n",
    "    \"\"\"\n",
    "    #Paso 1\n",
    "    D = [0.0 for teta in tetas] # Gradiente\n",
    "    \n",
    "    m = X.shape[1] # [1] porque cada columna es una observacion.\n",
    "    layers = len(nn_arch)\n",
    "    TETAS = build_tetas_matrix(T, nn_arch)\n",
    "    \n",
    "    # Paso 2.2\n",
    "    A = feed_forward(TETAS, X) # valores de activacion para cada neurona de cada capa\n",
    "    \n",
    "    # Calculo el error entre la hipotesis de la NN y los valores actuales.\n",
    "    d = [*range(layers - 1), A[-1] - Y] # -1 porque no tomo en cuenta la primera capa\n",
    "    \n",
    "    for l in range(layers-2, 0, -1): # -2 por capa de salida ya calculada, y capa de entrada. Solo para capas intermedias.\n",
    "        #print('Shape TETAS[l][:,1:] ', TETAS[l][:, 1:].T.shape)\n",
    "        #print('Shape Delta[l+1] ', D[l+1].shape)\n",
    "        #print('Shape A[l] ', A[l].shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        # Paso 2.4\n",
    "        d[l] = np.matmul(\n",
    "            TETAS[l][:, 1:].T, # (3, 3) Excluyo la neurona del bias.\n",
    "            d[l+1] # (3, m) cantidad de neuronas, m -> cantidad obs.\n",
    "        ) * A[l] * (1 - A[l])\n",
    "        \n",
    "        # Paso 2.5 y 2.6\n",
    "        D[l] += np.matmul(\n",
    "            d[l+1], # diferencia capa siguiente (3,5)\n",
    "            np.vstack((np.ones((1,m), float), A[l])).T # ()\n",
    "        )\n",
    "        \n",
    "    return np.divide(D, m) # Paso 3 retorno la matriz de gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
